{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Retrieval-Augmented\" Generation, [RAG](https://arxiv.org/pdf/2005.11401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA:\n",
    "\n",
    "- Separate knowledge from intelligence.\n",
    "- LLMs can be instruction tuned once, then they can be updated with new/ever changing knowledge, which may not be present in its training data\n",
    "- Large pre-trained language models store factual knowledge in their parameters.\n",
    "- These models achieve state-of-the-art results when fine-tuned on downstream NLP tasks.\n",
    "- However, their ability to access and manipulate knowledge is limited, affecting performance on knowledge-intensive tasks.\n",
    "- Provenance for decisions and updating world knowledge are still research challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag](./img/rag.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rag_requests...\n",
      "Loading llama_requests...\n",
      "Loading others...\n",
      "Loading done...\n"
     ]
    }
   ],
   "source": [
    "#print(\"Loading pipeline...\")\n",
    "#from llama_pipeline import get_llama_pipeline\n",
    "#from peft import PeftModel\n",
    "#print(\"Loading torch...\")\n",
    "#import torch\n",
    "print(\"Loading rag_requests...\")\n",
    "#from rag_utils import initialize_rag\n",
    "from rag_request import rag_request\n",
    "print(\"Loading llama_requests...\")\n",
    "from llama_request import llama_request\n",
    "\n",
    "print(\"Loading others...\")\n",
    "from threading import Thread\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "print(\"Loading done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(retrieved_docs: List[Document]) -> str:\n",
    "    \"\"\"Format retrieved documents into a context string.\"\"\"\n",
    "    context = \"Reference information:\\n\"\n",
    "    for doc, score in retrieved_docs:\n",
    "        content = doc[\"page_content\"]\n",
    "        source = doc[\"metadata\"].get(\"source\", \"Unknown\")\n",
    "        header = doc[\"metadata\"].get(\"header\", \"\")\n",
    "        \n",
    "        context += f\"\\n--- From {source}\"\n",
    "        if header:\n",
    "            context += f\" ({header})\"\n",
    "        context += f\" ---\\n{content}\\n\"\n",
    "    \n",
    "    context += \"\\nBased on the above information, please answer: \"\n",
    "    return context\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    \"\"\"Generate a streaming response using RAG and the fine-tuned model.\"\"\"\n",
    "    if not prompt:\n",
    "        return \"Hi I am an assistant for Candulor GmbH. I can help you with questions about their products. What do you need help with?\"\n",
    "    \n",
    "    # Retrieve relevant documents - changed from k=3 to k=5\n",
    "    retrieved_docs = rag_request(prompt, k=10, port=8001)\n",
    "    \n",
    "    # Format context\n",
    "    context = format_context(retrieved_docs)\n",
    "    \n",
    "    # Combine context and prompt\n",
    "    full_prompt = context + prompt\n",
    "        \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            #\"content\": \"You are a helpful AI assistant for Candulor GmbH. Answer questions based on the given reference information. If the information provided doesn't contain the answer, say you don't know.\"\n",
    "            \"content\": \"\"\"You are a very competent and helpful scholarly AI assistant.\n",
    "            You are an expert in most scholarly disciplines.\n",
    "            Your task is to answer questions about scientific topics based on reference information\n",
    "            from scientific papers that have been uploaded to arXiv.\n",
    "            You will be given several potentially relevant sections several papers, listed\n",
    "            with their file name (doc_00000000.md etc., which you should ignore), and their title.\n",
    "            If the references contain no useful information, answer 'I don't know, bro!'\n",
    "            If the question is about medicine, please answer 'I am not a doctor!'\n",
    "            Here are the five reference sections:\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ]\n",
    "    \n",
    "    \"\"\"text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\"\"\"\n",
    "\n",
    "    #generator = pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n",
    "\n",
    "    #chat = pipeline(messages, do_sample=True, max_new_tokens=512, temperature=0.7, top_p=0.9)[0] # [0] because we have only one chat\n",
    "\n",
    "    chat = llama_request(messages, port=8000)\n",
    "    \n",
    "    return chat\n",
    "    \n",
    "    \n",
    "    ## Create streamer\n",
    "    #streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_propmt=True)\n",
    "    \n",
    "    # Run generation in separate thread\n",
    "    #generation_kwargs = dict(\n",
    "    #    **model_inputs,\n",
    "    #    streamer=streamer,\n",
    "    #    max_new_tokens=512,\n",
    "    #    do_sample=True,\n",
    "    #    temperature=0.7,\n",
    "    #    top_p=0.9,\n",
    "    #)\n",
    "    \n",
    "    #thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    #thread.start()\n",
    "    \n",
    "    # Yield tokens as they're generated\n",
    "    #for new_text in streamer:\n",
    "    #    yield new_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"print(\"Initializing vector store...\")\n",
    "faiss_path = \"/data/horse/ws/s9650707-llm_secrets/datasets/unarxive/faiss_index3\"\n",
    "tmp_base_path = \"/tmp/s9650707/faiss/\"\n",
    "faiss_tmp_path = os.path.join(tmp_base_path, \"unarxive/faiss_index3\")\n",
    "\n",
    "if not os.path.exists(tmp_base_path):\n",
    "    os.makedirs(tmp_base_path)\n",
    "if not os.path.exists(faiss_tmp_path):\n",
    "    print(\"Copying faiss data to tmp\")\n",
    "    shutil.copytree(faiss_path, faiss_tmp_path)\n",
    "\n",
    "vector_store = initialize_rag(\n",
    "    markdown_dir=None,\n",
    "    faiss_index_file = faiss_tmp_path,\n",
    "    load_index_from_file=True,\n",
    "    store_index_to_file=False,\n",
    "    max_files=None\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"print(\"\\nLoading model...\")\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load both model and tokenizer from the fine-tuned output directory\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model_in_cache_name = f\"models--{model_name.split('/')[0]}--{model_name.split('/')[1]}\"\n",
    " \n",
    "cache_path = os.path.join(os.path.expanduser('~/.cache/huggingface/hub/'), model_in_cache_name, \"snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\")\n",
    "tmp_base_path = \"/tmp/s9650707/models/\"\n",
    "tmp_path = os.path.join(tmp_base_path, model_in_cache_name)\n",
    "if os.path.exists(cache_path):\n",
    "    print(\"Model exists in cache -> copy to tmp if necessary and use\")\n",
    "    if not os.path.exists(tmp_base_path):\n",
    "        os.makedirs(tmp_base_path)\n",
    "    if not os.path.exists(tmp_path):\n",
    "        print(\"Copying model to tmp\")\n",
    "        shutil.copytree(cache_path, tmp_path)\n",
    "    model_path = tmp_path\n",
    "else:\n",
    "    print(\"Model does not exist in cache -> download\")\n",
    "    model_path = model_name\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_path) #\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path) #\"Qwen/Qwen2-0.5B-Instruct\")  # Changed to load from fine-tuned path\n",
    "\n",
    "# Load LoRA weights\n",
    "#model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model = base_model\n",
    "model.to(device)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline = get_llama_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting ...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question. type 'quit' to exit. \n",
      "You:  What is SpiNNaker2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "sending request\n",
      "sending request\n",
      "SpiNNaker2 is a digital neuromorphic hardware system. It is a processing element (PE) architecture for hybrid digital neuromorphic computing, which is part of the second-generation SpiNNaker system (SpiNNaker 2).\n"
     ]
    }
   ],
   "source": [
    "def rag():\n",
    "    print(\"\\nStarting ...\")\n",
    "\n",
    "    # make interactive rag\n",
    "    while True:\n",
    "        prompt = input(\"Ask your question. type 'quit' to exit. \\nYou: \")\n",
    "        if prompt == \"quit\":\n",
    "            break\n",
    "        if not prompt:\n",
    "            print(\"Usage: python RAG.py <prompt>\")\n",
    "            sys.exit(1)\n",
    "        # Generate and stream response\n",
    "        print(\"\\nGenerating response...\\n\")\n",
    "        return generate_response(prompt)\n",
    "        #for token in generate_response_streaming(prompt, model, tokenizer, vector_store):\n",
    "        #    print(token, end=\"\", flush=True)\n",
    "        #print(\"\\n\")\n",
    "\n",
    "rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "- Study the code.\n",
    "- Add your own dataset.\n",
    "- Use your own local llm, run it in the hpc.\n",
    "- You may not use a finetuned model.\n",
    "- Change the code accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2\n",
    "- What can we do to make a improve ?\n",
    "- Write your own implementation of RAG\n",
    "- You can use your own template, your own dataset\n",
    "- end goal - make a chatbot that is tailored for one specific purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capella GPU Kernel LLM",
   "language": "python",
   "name": "capella_gpu_kernel_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
