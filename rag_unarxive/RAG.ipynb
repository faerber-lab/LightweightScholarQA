{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Retrieval-Augmented\" Generation, [RAG](https://arxiv.org/pdf/2005.11401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA:\n",
    "\n",
    "- Separate knowledge from intelligence.\n",
    "- LLMs can be instruction tuned once, then they can be updated with new/ever changing knowledge, which may not be present in its training data\n",
    "- Large pre-trained language models store factual knowledge in their parameters.\n",
    "- These models achieve state-of-the-art results when fine-tuned on downstream NLP tasks.\n",
    "- However, their ability to access and manipulate knowledge is limited, affecting performance on knowledge-intensive tasks.\n",
    "- Provenance for decisions and updating world knowledge are still research challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag](./img/rag.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from threading import Thread\n",
    "import sys\n",
    "from rag_utils import initialize_rag\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(retrieved_docs: List[Document]) -> str:\n",
    "    \"\"\"Format retrieved documents into a context string.\"\"\"\n",
    "    context = \"Reference information:\\n\"\n",
    "    for doc in retrieved_docs:\n",
    "        content = doc.page_content\n",
    "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        header = doc.metadata.get(\"header\", \"\")\n",
    "        \n",
    "        context += f\"\\n--- From {source}\"\n",
    "        if header:\n",
    "            context += f\" ({header})\"\n",
    "        context += f\" ---\\n{content}\\n\"\n",
    "    \n",
    "    context += \"\\nBased on the above information, please answer: \"\n",
    "    return context\n",
    "\n",
    "def generate_response_streaming(prompt: str, model, tokenizer, vector_store):\n",
    "    \"\"\"Generate a streaming response using RAG and the fine-tuned model.\"\"\"\n",
    "    if not prompt:\n",
    "        return \"Hi I am an assistant for Candulor GmbH. I can help you with questions about their products. What do you need help with?\"\n",
    "    \n",
    "    # Retrieve relevant documents - changed from k=3 to k=5\n",
    "    retrieved_docs = vector_store.similarity_search(prompt, k=5)\n",
    "    \n",
    "    # Format context\n",
    "    context = format_context(retrieved_docs)\n",
    "    \n",
    "    # Combine context and prompt\n",
    "    full_prompt = context + prompt\n",
    "        \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a helpful AI assistant for Candulor GmbH. Answer questions based on the given reference information. If the information provided doesn't contain the answer, say you don't know.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Create streamer\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "    \n",
    "    # Run generation in separate thread\n",
    "    generation_kwargs = dict(\n",
    "        **model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    # Yield tokens as they're generated\n",
    "    for new_text in streamer:\n",
    "        yield new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vector store...\n",
      "\n",
      "Creating Vector Store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/s9650707-llm_workspace/workspace/09_RAG_Tools/rag_utils.py:134: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name, show_progress=True if docs_per_batch is None else False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the vector store from file\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing vector store...\")\n",
    "vector_store = initialize_rag(\n",
    "    markdown_dir=\"./dataset/md/\",\n",
    "    faiss_index_file = \"/data/horse/ws/s9650707-llm_secrets/datasets/unarxive/faiss_index\",\n",
    "    load_index_from_file=True,\n",
    "    store_index_to_file=False,\n",
    "    max_files=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model...\n",
      "Using device: cuda\n",
      "\n",
      "Starting ...\n"
     ]
    }
   ],
   "source": [
    "def rag(vector_store):\n",
    "\n",
    "    print(\"\\nLoading model...\")\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load both model and tokenizer from the fine-tuned output directory\n",
    "    model_path = \"./finetuned\"\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")  # Changed to load from fine-tuned path\n",
    "\n",
    "    # Load LoRA weights\n",
    "    #model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = base_model\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"\\nStarting ...\")\n",
    "\n",
    "    # make interactive rag\n",
    "    while True:\n",
    "        prompt = input(\"Ask your question. type 'quit' to exit. \\nYou: \")\n",
    "        if prompt == \"quit\":\n",
    "            break\n",
    "        if not prompt:\n",
    "            print(\"Usage: python RAG.py <prompt>\")\n",
    "            sys.exit(1)\n",
    "        # Generate and stream response\n",
    "        print(\"\\nGenerating response...\\n\")\n",
    "        for token in generate_response_streaming(prompt, model, tokenizer, vector_store):\n",
    "            print(token, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "\n",
    "rag(vector_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "- Study the code.\n",
    "- Add your own dataset.\n",
    "- Use your own local llm, run it in the hpc.\n",
    "- You may not use a finetuned model.\n",
    "- Change the code accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2\n",
    "- What can we do to make a improve ?\n",
    "- Write your own implementation of RAG\n",
    "- You can use your own template, your own dataset\n",
    "- end goal - make a chatbot that is tailored for one specific purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capella GPU Kernel LLM",
   "language": "python",
   "name": "capella_gpu_kernel_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
